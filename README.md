
# 100 Days of ML
<img alt="GitHub" src="https://img.shields.io/github/license/Harsh188/100_Days_of_ML"> <img alt="GitHub top language" src="https://img.shields.io/github/languages/top/Harsh188/100_Days_of_ML">

Daily log to track my progress on the 100 days of ML code challenge.

## Description
100 Day ML Challenge to learn and implement ML/DL concepts ranging from the basics to more advanced state of the art models.

## Daily Logs
<h3>Day 1 [09/09/20]: Multivariate Linear Regression</h4>
<ul>
	<li>Started <a href="">Machine Learning by Stanford University</a> course on Coursera.</li>
	<li>Utilized TensorFlow tensors for matrix multiplication.</li>
	<li>Created an informative notebook for multivariate regression.</li>
</ul>
<h3>Day 2 [10/09/20]: Applying Regression</h3>
<ul>
	<li>Used the Seoul Bike Sharing Demand dataset found at <a href="https://archive.ics.uci.edu/ml/datasets.php?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table">UCI Machine Learning Repository</a> for multivariate regressio.n</li>
	<li>Utilized the Keras library through TensorFlow.</li>
	<li>Used a Sequential model with two hidden layers.</li>
</ul>
<h3>Day 3 [13/09/20]: Custom Regression Model</h3>
<ul>
	<li>Building a custom hand tuned regression model based on previous results.</li>
	<li>Trained using basic matrix operations and Adam optimizer</li>
	<li>Watched Stanford's CS229 lecture on <a href="https://www.youtube.com/watch?v=4b4MUYve_U8&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=2">Linear Regression and Gradient Decent</a> taught by Andrew Ng.</li>
</ul>
<h3>Day 4 [14/09/2020]: Generative Discriminative</h3>
<ul>
	<li>Watched Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=nt63k3bfXS0&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=5">GDA & Naive Bayes.</a></li>
	<li>Noted the difference between Generative and Discriminative models.</li>
</ul>
<h3>Day 5 [15/09/20]: Naive Bayes</h3>
<ul>
	<li>Started a mini-project on classifying Iris flowers using Naive Bayes.</li>
	<li>Learned a lot about Naive Bayes through several videos on youtube such as
		<ul>
			<li><a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">Bayes theorem.</a></li>
			<li><a href="https://www.youtube.com/watch?v=O2L2Uv9pdDA">Naive Bayes, Clearly Explained!!!</a></li>
			<li><a href="https://www.youtube.com/watch?v=H3EjCKtlVog">Gaussian Naive Bayes, Clearly Explained!!!</a></li>
		</ul>
	</li>
</ul>
<h3>Day 6 [16/09/20]: Naive Bayes Project</h3>
<ul>
	<li>Finished the Iris Flower Classifier using Naive Bayes.</li>
	<li>Reached an accuracy of about 96%</li>
</ul>
<h3>Day 7 [17/09/20]: Support Vector Machines.</h3>
<ul>
	<li>Learned a lot about Support Vector Machines by watching several videos on youtube such as</li>
	<ul>
		<li>Stanford's CS299 lecture on <a href="https://youtu.be/lDwow4aOrtg?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&t=2788">Support Vector Machines.</a></li>
		<li><a href="https://www.youtube.com/watch?v=efR1C6CvhmE">Support Vector Machines, Clearly Explained!!!</a></li>
		<li>MIT 6.034 Artificial Intelligence lecture 16 on <a href="https://www.youtube.com/watch?v=_PwhiWxHK8o&t=1s">Learning: Support Vector Machines.</a></li>
	</ul>
</ul>
<h3>Day 8 [18/09/20]: SVM Project</h3>
<ul>
	<li>Started a project on classifying <a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)">Breast Cancer Tumors </a>using SVM.</li>
	<li>Followed a <a href="https://www.youtube.com/watch?v=mA5nwGoRAOo">tutorial</a> on youtube by Sentdex on SVM.</li>
	<li>Received and accuracy in the range of around 97%</li>
</ul>
<h3>Day 9 [19/09/20]: Classification</h3>
<ul>
	<li>Going back to the basics and approaching classification from a mathematical standpoint.</li>
	<li>Completed the Classification and Representation section in the <a href="https://www.coursera.org/learn/machine-learning">Machine Learning</a> course by Stanford on coursera.</li>
</ul>
<h3>Day 10 [20/09/20] Kernels</h3>
<ul>
	<li>Watched Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=8NYoQiRANpg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=8&t=0s">Kernels.</a></li>
	<li>Learned the representer theorem.</li>
	<li></li>
</ul>
<h3>Day 11 [21/09/20] Kernels continued.</h3>
<ul>
	<li>Finished the Stanford CS299 lecture on <a href="https://www.youtube.com/watch?v=8NYoQiRANpg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=8&t=0s">Kernels.</a></li>
	<li>Learned about the complexity differnece when using inner product.</li>
	<!-- <li></li> -->
</ul>
<h3>Day 12 [23/09/20] Bias and Variance</h3>
<ul>
	<li>Watched Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=rjbkWSTjHzM&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=8">Data Splits, Models & Cross-Validation.</a></li>
	<li>Learned about</li>
	<ul>
		<li>Overfitting and underfitting in terms of bias and variance.</li>
		<li>The regularization technique.</li>
	</ul>
	
</ul>