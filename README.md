
# 100 Days of ML
<img alt="GitHub" src="https://img.shields.io/github/license/Harsh188/100_Days_of_ML"> <img alt="GitHub top language" src="https://img.shields.io/github/languages/top/Harsh188/100_Days_of_ML">

Daily log to track my progress on the 100 days of ML code challenge.

## Description
100 Day ML Challenge to learn and implement ML/DL concepts ranging from the basics to more advanced state of the art models.

## Daily Logs
<h3>Day 1 [09/09/20]: Multivariate Linear Regression</h4>
<ul>
	<li>Started <a href="">Machine Learning by Stanford University</a> course on Coursera.</li>
	<li>Utilized TensorFlow tensors for matrix multiplication.</li>
	<li>Created an informative notebook for multivariate regression.</li>
</ul>
<h3>Day 2 [10/09/20]: Applying Regression</h3>
<ul>
	<li>Used the Seoul Bike Sharing Demand dataset found at <a href="https://archive.ics.uci.edu/ml/datasets.php?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table">UCI Machine Learning Repository</a> for multivariate regressio.n</li>
	<li>Utilized the Keras library through TensorFlow.</li>
	<li>Used a Sequential model with two hidden layers.</li>
</ul>
<h3>Day 3 [13/09/20]: Custom Regression Model</h3>
<ul>
	<li>Building a custom hand tuned regression model based on previous results.</li>
	<li>Trained using basic matrix operations and Adam optimizer</li>
	<li>Watched Stanford's CS229 lecture on <a href="https://www.youtube.com/watch?v=4b4MUYve_U8&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=2">Linear Regression and Gradient Decent</a> taught by Andrew Ng.</li>
</ul>
<h3>Day 4 [14/09/2020]: Generative Discriminative</h3>
<ul>
	<li>Watched Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=nt63k3bfXS0&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=5">GDA & Naive Bayes.</a></li>
	<li>Noted the difference between Generative and Discriminative models.</li>
</ul>
<h3>Day 5 [15/09/20]: Naive Bayes</h3>
<ul>
	<li>Started a mini-project on classifying Iris flowers using Naive Bayes.</li>
	<li>Learned a lot about Naive Bayes through several videos on youtube such as
		<ul>
			<li><a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">Bayes theorem.</a></li>
			<li><a href="https://www.youtube.com/watch?v=O2L2Uv9pdDA">Naive Bayes, Clearly Explained!!!</a></li>
			<li><a href="https://www.youtube.com/watch?v=H3EjCKtlVog">Gaussian Naive Bayes, Clearly Explained!!!</a></li>
		</ul>
	</li>
</ul>
<h3>Day 6 [16/09/20]: Naive Bayes Project</h3>
<ul>
	<li>Finished the Iris Flower Classifier using Naive Bayes.</li>
	<li>Reached an accuracy of about 96%</li>
</ul>
<h3>Day 7 [17/09/20]: Support Vector Machines.</h3>
<ul>
	<li>Learned a lot about Support Vector Machines by watching several videos on youtube such as</li>
	<ul>
		<li>Stanford's CS299 lecture on <a href="https://youtu.be/lDwow4aOrtg?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&t=2788">Support Vector Machines.</a></li>
		<li><a href="https://www.youtube.com/watch?v=efR1C6CvhmE">Support Vector Machines, Clearly Explained!!!</a></li>
		<li>MIT 6.034 Artificial Intelligence lecture 16 on <a href="https://www.youtube.com/watch?v=_PwhiWxHK8o&t=1s">Learning: Support Vector Machines.</a></li>
	</ul>
</ul>
<h3>Day 8 [18/09/20]: SVM Project</h3>
<ul>
	<li>Started a project on classifying <a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)">Breast Cancer Tumors </a>using SVM.</li>
	<li>Followed a <a href="https://www.youtube.com/watch?v=mA5nwGoRAOo">tutorial</a> on youtube by Sentdex on SVM.</li>
	<li>Received and accuracy in the range of around 97%</li>
</ul>
<h3>Day 9 [19/09/20]: Classification</h3>
<ul>
	<li>Going back to the basics and approaching classification from a mathematical standpoint.</li>
	<li>Completed the Classification and Representation section in the <a href="https://www.coursera.org/learn/machine-learning">Machine Learning</a> course by Stanford on coursera.</li>
</ul>
<h3>Day 10 [20/09/20] Kernels</h3>
<ul>
	<li>Watched Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=8NYoQiRANpg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=8&t=0s">Kernels.</a></li>
	<li>Learned the representer theorem.</li>
	<li></li>
</ul>
<h3>Day 11 [21/09/20] Kernels continued.</h3>
<ul>
	<li>Finished the Stanford CS299 lecture on <a href="https://www.youtube.com/watch?v=8NYoQiRANpg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=8&t=0s">Kernels.</a></li>
	<li>Learned about the complexity differnece when using inner product.</li>
	<!-- <li></li> -->
</ul>
<h3>Day 12 [23/09/20] Bias and Variance</h3>
<ul>
	<li>Watched Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=rjbkWSTjHzM&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=8">Data Splits, Models & Cross-Validation.</a></li>
	<li>Learned about</li>
	<ul>
		<li>Overfitting and underfitting in terms of bias and variance.</li>
		<li>The regularization technique.</li>
	</ul>
</ul>
<h3>Day 13 [24/09/2020] Cross-Validation</h3>
<ul>
	<li>Finished watching the CS299 lecture on Cross Validation.</li>
	<li>Learned about</li>
	<ul>
		<li>How and when to use k-fold cross validation.</li>
		<li>How and when to use leave-out-out cross validation.</li>
		<li>Feature selection.</li>
	</ul>
</ul>
<h3>Day 14 [25/09/2020] Approx/Estimation Error</h3>
<ul>
	<li>Watched Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=iVOxMcumR4A&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=9">Approx/Estimation Error</a>.</li>
	<li>Learned about</li>
	<ul>
		<li>Sampling Distributions</li>
		<li>Parameter View</li>
		<li>Bayes Error</li>
		<li>Approximation Error</li>
		<li>Estimation Error</li>
	</ul>
</ul>
<li>Day 15 [26/09/2020] Emprical Risk Minimization</li>
<ul>
	<li>Finished up CS299 lecture on <a href="https://www.youtube.com/watch?v=iVOxMcumR4A&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=9&t=2680s">ERM.</a></li>
	<li>Uniform convergence</li>
</ul>
<h3>Day 16 [27/09/2020] Decision Trees</h3>
<ul>
	<li>Started watching Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=wr9gUr-eWdA&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=10">Decision Trees and Ensemble Methods.</a></li>
	<li>Missclassificaiton and its issues with predicting the differences in certain cases.</li>
	<li>How cross-entropy tackles the downfall of missclassificaiton loss.</li>
</ul>
<h3>Day 17 [28/09/2020] Decision Trees Cont.</h3>
<ul>
	<li>Continued Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=wr9gUr-eWdA&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=10">Decision Trees and Ensemble Methods.</a></li>
	<li>Regression Trees.</li>
	<li>Regularization of Decision Trees.</li>
	<li>Runtime for Decision Trees.</li>
	<li>Advantages and disadvantages of decision trees.</li>
</ul>
<h3>Day 18 [29/09/2020] Ensemble Methods</h3>
<ul>
	<li>Finished up Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=wr9gUr-eWdA&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=10">Decision Trees and Ensemble Methods.</a></li>
	<li>How to combine differnt learning algorithms and average their results.</li>
	<li>How to utilize different training sets.</li>
</ul>
<h3>Day 19 [30/09/2020] Decision Trees Mini Project</h3>
<ul>
	<li>Implemented decision trees on the iris dataset from UC Irvine Machine Learning Repository.</li>
	<li>Recieved and accuracy of ~97%.</li>
</ul>
<h3>Day 20 [01/09/2020] Neural Networks</h3>
<ul>
	<li>Started Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=MfIjxPh6Pys&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=11&t=1854s">Introduction to Neural Networks.</a></li>
	<li>Learned about:</li>
	<ul>
		<li>Equational form of neurons and models.</li>
		<li>Neural networks as a form of linear regression.</li>
		<li>Softmax</li>
	</ul>
</ul>
<h3>Day 21 [02/09/2020] Neural Networks cont.</h3>
<ul>
	<li>Continued Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=MfIjxPh6Pys&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=11&t=1854s">Introduction to Neural Networks.</a></li>
	<li>Learned about:</li>
	<ul>
		<li>End to end learning</li>
		<li>Black box models</li>
		<li>Propogation equations</li>
	</ul>
</ul>
<h3>Day 22 [03/10/2020] Dense Neural Network Mini Project</h3>
<ul>
	<li>Trained neural network model to classify images of clothing.</li>
	<li>Utilized Fashion MNIST dataset.</li>
	<li>Followed the <a href="https://www.tensorflow.org/tutorials/keras/classification">TensorFlow guide.</a></li>
</ul>
<h3>Day 23 [04/10/2020] Backprop</h3>
<ul>
	<li>Started Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=zUazLXZZA2U&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=12&t=1s">Backprop & Improving Neural Networks.</a></li>
	<li>Learned how to improved Neural Networks.</li>
	<li>Vanishing/Exploding Gradient problem.</li>
</ul>
<h3>Day 24 [05/10/2020] Debugging ML Models</h3>
<ul>
	<li>Started Stanford's CS299 lecture on <a href="https://www.youtube.com/watch?v=ORrStCArmP4&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=13">Debugging ML Models and Error Analysis.</a></li>
	<li>Methos to fixing the learning algorithm.</li>
</ul>
<h3>Day 25 [06/10/2020] Neural Networks: Representation</h3>
<ul>
	<li>Week 4 of <a href="">Machine Learning</a> course on coursera.</li>
	<li>Non-linear Hypotheses.</li>
	<li>Neurons and the Brain.</li>
</ul>