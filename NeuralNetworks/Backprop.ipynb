{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "In order to define our optimization problem we need to define our cost function. After doing this we need to update the layers with the derivative of the loss function. We will start from the top most neuron and work backwords since this will make the computation easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Neural Networks\n",
    "1. Use different activation functions\n",
    "2. Initialization methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need activation functions?\n",
    "The purpose of the activation function is to introduce non-linearity into the output of a neuron. [GeeksforGeeks](https://www.geeksforgeeks.org/activation-functions-neural-networks/#:~:text=The%20purpose%20of%20the%20activation,the%20output%20of%20a%20neuron.&text=We%20know%2C%20neural%20network%20has,and%20their%20respective%20activation%20function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are initialization methods?\n",
    "We can normalize our input. It helps our loss function become easier to train and allow the gradient decent work faster. The way you inititalize your weights is really important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients\n",
    "All the errors end up multiplying. One way to avoid this is by initializing your weights properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
