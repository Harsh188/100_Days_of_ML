{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "According to the definition given on [GeeksforGeeks](https://www.geeksforgeeks.org/decision-tree/):\n",
    "\n",
    "**Decision tree** is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose splits?\n",
    "### Misclassification Loss\n",
    "Define L(R): loss function on R. Given C classes, define p hat to be the proportion of examples in R that are of class c. $$ L (missclass) = I - max p hat $$ \n",
    "#### Issues\n",
    "It turns out that in certain cases where one option would seem better than the other, the missclassification loss turns out to be the same. In order to do this we instead define **cross-entropy** loss. $$ Lcross = summation( p hat log(p hat) ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "Decision trees are used for classification but we could also modify it and use it for regression. These trees are known as **regression trees**.\n",
    "\n",
    "Regression trees work similar to decision trees, but instead of predicting the majority class you can predict the mean of the values left. To calculate the loss in this case we will used squared mean error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "There are differnt ways you can regularize decision trees. \n",
    "1. If you hit a minimum leaf size stop splitting.\n",
    "2. Enforce a max depth.\n",
    "3. Have a max number of nodes.\n",
    "4. Define a minimum decrease in loss.\n",
    "5. Pruning (misclassificaiton with a validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime\n",
    "\n",
    "\n",
    "n examples;\n",
    "f features;\n",
    "d depth of tree\n",
    "\n",
    "During **test time** your runtime is O(d). Typically d<log(n).\n",
    "\n",
    "During **train time** each point is part of O(d) nodes. So the cost of point at each node is O(f).\n",
    "\n",
    "Therefore the **total cost** is O(nfd)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages\n",
    "**Advantages**\n",
    "- Easy to explain\n",
    "- Fast\n",
    "- Able to handle categorical variables and continuous variables.\n",
    "\n",
    "**Disadvantages**\n",
    "- They have high variance.\n",
    "- Bad at additive data.\n",
    "- Generally have low predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
